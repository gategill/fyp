\section{Recommender Systems in General}
A recommender system is simply a program that provides a user with items they might like based on context that the user has previously liked. Collaborative filtering has been one of the most popular and effective techniques[]. 

Recommender systems have come a long way since KNN. 

"Content-based and collaborative filtering used different types of data to arrive at a filtering decision, Content-filtering tools select the right information for the right people by comparing representation of content contained in the documents to representation of content that the user in interested in. Contest-based information filtering has proven to be effective in selection textual documents relevant to a topic using techniques such as vector-space queries, intelligent agent and information visualisation"

problems

approaches, 
+ve and -ve

\section{Information Overload}
Studies even suggest that more choice negatively impacts attention \cite{attention}
Psychologists agree that the human mind has a limited capacity for attention and that we make poor decisions when overloaded.

The term was coined by Bertram Gross, professor of political science, in his 1964 work, The Managing of Organizations []. Since,  research has come out suggesting that information overload results in lower productivity[], decision fatigue[] and decrease in attention span[]. 


The amount of data we produce doubles around 18 months in a trend resembling Moore's Law and it is estimated that by 2025, it will grow to 175 zettabytes, that's 10 to the power of 21. By comparison, there is around one Zetta stars in the universe. Although the increase of computing power in computer can keep up with this exponential pace of increase, our minds can't. This means that as a consumer, there is more and more to choose from on a daily basis yet it's increasingly difficult to to discover useful information. 
However more choice is not always good. Decision fatigue and social media = stress. 

Even though the amount of information available has increased, the problem of sparsity has not


\section{Co-Training}
Machine learning models are generally split into 4 major categories: supervised, unsupervised, semisupervised and reinforcement learning. 
Recommender systems usually fall under semisupervised and reinforcement learning.

Semisupervised learning where we have a large volume of unlabelled data that includes a small volume of labelled data. This category introduces the possibility of (). This is where co-training comes into play. We can use the labelled data to fit another model that will label the unlabelled data for us and therefore (). This technique is also sometimes called imputing. There are many ways to approach this is interesting because.

One of the main challenges of any machine learning model is insufficient quantity of training data. Many models learn from large volumes of labelled data. The model generalises to this data and makes predictions to examples it has never seen before. If not enough(), quality data is provided, the approximations will be poor and tend to overfit or underfit.

Co-training can be applied to recommender systems, where the shortage of labelled data is a well-known problem \cite{corec}. 

\section{Elliot}
For this project, I used the open-source framework Elliot[] to create, test and compare models. Elliot lets us load and filter data, test models, find hyperparameters and compute metrics by providing a simple configuration file. Elliot plays an important role in ensuring that the experiment is reproducible and thus verifiable and comparable to other research.


\section{Challenges}
Another challenge in recommender systems is that the baseline model (a most popular recommender) is hard to beat. Although this problem is not unique to recommender systems, when modelling infection rates, the baseline (horizontal line) often outperforms complex models[]

Current approaches and algorithms and their limitations, that gender bias


Inconsistencies when testing models = Elliot

Outdated metrics







