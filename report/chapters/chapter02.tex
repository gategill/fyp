Related Work
other approaches, describe problems, metrics old to new, 


\section{Section 1 of Chapter 2}
Different approaches exist, such as collaborative and content-based approaches. In collaborative approaches, we assume that users who agreed in the past will agree in the future and by grouping a user with others with similar tastes we can score items unseen to the user and provide a recommendation. In content-based approaches, AAA
The most commonly addressed problems in the literature are sparsity and cold start. Within a recommender system, the number of items is very large and most users would interact with a small subset of the items. This means that only a small percentage of items will be rated by users. This presents a problem as there isnâ€™t enough information to train a model accurately.  

because any learner system needs sufficient data to perform optimally, otherwise it runs into the risk of overfitting and capturing noise instead of the true underlying pattern. Sparsity is a common problem for collaborative recommenders. To fix it, a human would have to label thousands and perhaps millions of points manually. Because of this, we observe a tail distribution of ratings[]. Co-training aims to fix this by making two recommenders train each other by adding their most confident predictions on the unlabelled data to the training set iteratively.


Cold starts refer to the difficulty in providing an accurate recommendation for a new item or user. 

 There are several ways it has been approached.  for example[] 

The methods of evaluating recommender systems have also evolved. Instead of trying to predict a rating that a user might assign to an item and measuring the mean absolute error, an entire fan of new methods is used to better evaluate models. Instead of focusing on predicting a rating, they try to <sort> items. This way, we can evaluate novelty, diversity, top N, NG 
