\thispagestyle{plain}

\begin{center}
    \Large
    \textbf{Co-Training Recommender Systems}
    
    \vspace{0.4cm}
    \large
     CoRec: A Co-Training Approach \\ for Recommender Systems
    
    \vspace{0.4cm}
    
    \textbf{Michal Krzysztof \\ Polak Szarkowicz}
    
    \vspace{0.4cm} 
    
    \textbf{118304271}
    
    \vspace{0.9cm}
     
    \textbf{Abstract}
\end{center}
 
We come in contact with recommender systems daily thought services like Netflix, Spotify and possibly soon, McDonald’s[]. Recommenders help users to navigate the sea of endless content by serving them what they already like, as well as what they might potentially like. 
\\~\\
The amount of data we produce doubles around 18 months in a trend resembling Moore's Law. This means that as a consumer, there is more and more to choose from on a daily basis yet it's increasingly difficult to to discover useful information. 
\\~\\
Thanks to recent advances and more data[], they are better than ever before at providing personalised suggestions[], and it pays off. Back in 2007, Netflix hosted the Netflix Prize, in which it offered 1 million dollars to the team that improved their recommender system the most. To say it was worth it is an under statement. The average Netflix user spends almost 9 hours a week on the platform. Also, let's not forget that around a third of Amazon’s revenue comes from its recommendations[]. 
\\~\\
However, an important issue that persists, such as sparsity. This is where a large number of points are unlabelled due to the large space of items and users and their interactions. By using a technique called co-training, where we use existing ratings to enhance the training dataset and thus producing more labelled ratings for a model to work with, we can produce better results and reduce the impact of sparsity.
\\~\\
In this project, we outline several recommender systems that aim to alleviate the problem of sparse, try out different recommender algorithms, we will try different datasets and try different ways of measuring uncertainty.

